{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933cbdf0",
   "metadata": {},
   "source": [
    "## Enhanced RAG with Reranking\n",
    "\n",
    "This notebook demonstrates how to apply reranking methods to improve retrieval accuracy in RAG systems.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. Document Loader \n",
    "2. Text Splitter\n",
    "3. Embedding Model\n",
    "4. Vector Store\n",
    "5. **Two-Stage Retrieval**:\n",
    "   - Initial Retrieval (Dense/Sparse)\n",
    "   - **Reranking with Cross-Encoder**\n",
    "6. Prompt Template\n",
    "7. LLM \n",
    "8. Enhanced Chain with Reranking\n",
    "9. Evaluation with Reranking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a67d9464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"/Users/reejungkim/Documents/Git/working-in-progress/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca58ff",
   "metadata": {},
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install sentence-transformers faiss-cpu langchain-community langchain-groq -q„Ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc76e3",
   "metadata": {},
   "source": [
    "### 1. Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ce8080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 91 pages from PDF\n",
      "\n",
      "Sample content from page 1:\n",
      "ANNUAL REPORT\n",
      "2 0 2 4...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load your PDF file\n",
    "file_path = \"/Users/reejungkim/Documents/Git/AI-Agent/Amazon-2024-Annual-Report.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages from PDF\")\n",
    "\n",
    "# Show sample content\n",
    "if docs:\n",
    "    print(f\"\\nSample content from page 1:\")\n",
    "    print(docs[0].page_content[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69cec7",
   "metadata": {},
   "source": [
    "### 2. Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fdabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 425 chunks\n",
      "\n",
      "Sample chunk:\n",
      "ANNUAL REPORT\n",
      "2 0 2 4...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Text splitter configuration\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(texts)} chunks\")\n",
    "\n",
    "# Show sample chunk\n",
    "if texts:\n",
    "    print(f\"\\nSample chunk:\")\n",
    "    print(texts[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fe75f",
   "metadata": {},
   "source": [
    "### 3. Embedding Model\n",
    "\n",
    "Using the user's preferred embedding model: `all-MiniLM-L6-v2` for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb75d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model (user's preferred model for speed)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',  # User's preference\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"Embedding model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf75a3",
   "metadata": {},
   "source": [
    "### 4. Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8233b0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "Vector store created in 31.16 seconds\n",
      "Indexed 425 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create vector store\n",
    "print(\"Creating vector store...\")\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore = FAISS.from_documents(texts, embedding_model)\n",
    "\n",
    "creation_time = time.time() - start_time\n",
    "print(f\"Vector store created in {creation_time:.2f} seconds\")\n",
    "print(f\"Indexed {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93889c",
   "metadata": {},
   "source": [
    "### 5. Reranking Setup\n",
    "\n",
    "This is the key enhancement - adding a reranking layer using cross-encoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7297698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Reranker Models:\n",
      "- ms-marco-miniLM-L6-v2: Fast and balanced model for general reranking (Speed: fast, Accuracy: good)\n",
      "- ms-marco-miniLM-L12-v2: Better accuracy with moderate speed (Speed: medium, Accuracy: very good)\n",
      "- ms-marco-TinyBERT-L6: Fastest model with decent accuracy (Speed: very fast, Accuracy: good)\n",
      "- qnli-electra-base: Specialized for question-answering tasks (Speed: medium, Accuracy: very good)\n"
     ]
    }
   ],
   "source": [
    "# Import the reranking module we created\n",
    "from rerank_module import (\n",
    "    DocumentReranker, \n",
    "    RerankConfig, \n",
    "    RetrievalWithReranking,\n",
    "    RERANKER_MODELS\n",
    ")\n",
    "\n",
    "# Display available reranker models\n",
    "print(\"Available Reranker Models:\")\n",
    "for name, info in RERANKER_MODELS.items():\n",
    "    print(f\"- {name}: {info['description']} (Speed: {info['speed']}, Accuracy: {info['accuracy']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbbd8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranker model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1354f6706c4e688b0d2bcabb06fc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0af7a23e4f4ee9848ea85cc8fc4f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712ab64f99394b4d86db48e22e073011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d43b9dcc3a4c92ada4e1ffb357226e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7148d0a66854ee0abebb54a2721e362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a64411ce43642789cb6d9bb53914144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bff8e3961854bda9e2ac6c79a421928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker loaded successfully in 23.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize reranker with a fast model (user preference)\n",
    "rerank_config = RerankConfig(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L6-v2\",  # Fast and balanced\n",
    "    threshold=0.0,  # Include all documents above this score\n",
    "    max_length=512,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Loading reranker model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "reranker = DocumentReranker(rerank_config)\n",
    "\n",
    "if reranker.is_available():\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"Reranker loaded successfully in {load_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"Reranker not available. Please install sentence-transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25534eaa",
   "metadata": {},
   "source": [
    "### 6. Compare Regular Retrieval vs Reranked Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1063563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: How much cash was used in operating activities in 2024?\n",
      "\n",
      "=== REGULAR RETRIEVAL ===\n",
      "Retrieved 5 documents in 0.076s\n",
      "\n",
      "1. currency balances include British Pounds, Canadian Dollars, Euros, Indian Rupees, and Japanese Yen. \n",
      "Cash provided by (used in) operating activities was $84.9 billion and $115.9 billion in 2023 and 20...\n",
      "\n",
      "2. Consolidated Statements of Cash Flows Reconciliation\n",
      "The following table provides a reconciliation of the amount of cash, cash equivalents, and restricted cash reported within \n",
      "the consolidated balanc...\n",
      "\n",
      "3. adequacy of our tax accruals. Although we believe our tax estimates are reasonable, the final outcome of audits, investigations, \n",
      "and any other tax controversies could be materially different from our...\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== RERANKED RETRIEVAL ===\n",
      "Reranked 10 documents, returned 5 in 0.858s\n",
      "\n",
      "1. [Score: 9.337, Rank: 1, Orig: 1]\n",
      "   currency balances include British Pounds, Canadian Dollars, Euros, Indian Rupees, and Japanese Yen. \n",
      "Cash provided by (used in) operating activities was $84.9 billion and $115.9 billion in 2023 and 20...\n",
      "\n",
      "2. [Score: 8.879, Rank: 2, Orig: 8]\n",
      "   cash provided by (used in) operating activities,‚Äù for 2023 and 2024 (in millions):\n",
      " Year Ended December 31,\n",
      " 2023 2024\n",
      "Net cash provided by (used in) operating activities $ 84,946 $ 115,877 \n",
      "Purchases...\n",
      "\n",
      "3. [Score: 7.848, Rank: 3, Orig: 4]\n",
      "   have been leased. The following is a reconciliation of free cash flow less equipment finance leases and principal repayments of \n",
      "all other finance leases and financing obligations to the most comparab...\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "test_query = \"How much cash was used in operating activities in 2024?\"\n",
    "\n",
    "print(f\"Test Query: {test_query}\\n\")\n",
    "\n",
    "# Regular retrieval\n",
    "print(\"=== REGULAR RETRIEVAL ===\")\n",
    "start_time = time.time()\n",
    "regular_docs = vectorstore.similarity_search(test_query, k=5)\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "print(f\"Retrieved {len(regular_docs)} documents in {regular_time:.3f}s\")\n",
    "for i, doc in enumerate(regular_docs[:3]):\n",
    "    print(f\"\\n{i+1}. {doc.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Reranked retrieval\n",
    "print(\"=== RERANKED RETRIEVAL ===\")\n",
    "if reranker.is_available():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get more documents initially for reranking\n",
    "    initial_docs = vectorstore.similarity_search(test_query, k=10)\n",
    "    \n",
    "    # Rerank and get top 5\n",
    "    reranked_docs = reranker.rerank_documents_simple(\n",
    "        test_query, \n",
    "        initial_docs, \n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    rerank_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Reranked {len(initial_docs)} documents, returned {len(reranked_docs)} in {rerank_time:.3f}s\")\n",
    "    for i, doc in enumerate(reranked_docs[:3]):\n",
    "        score = doc.metadata.get('rerank_score', 'N/A')\n",
    "        position = doc.metadata.get('rerank_position', 'N/A')\n",
    "        orig_pos = doc.metadata.get('original_position', 'N/A')\n",
    "        print(f\"\\n{i+1}. [Score: {score:.3f}, Rank: {position}, Orig: {orig_pos}]\")\n",
    "        print(f\"   {doc.page_content[:200]}...\")\n",
    "else:\n",
    "    print(\"Reranker not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57333fda",
   "metadata": {},
   "source": [
    "### 7. LLM Setup\n",
    "\n",
    "Using the user's preferred LLM model: `llama3-8b-8192` for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully\n",
      "LLM test response: I'm functioning properly, thank you for asking. I'm a large language model, so I don't have emotions...\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize LLM (user's preferred model for speed)\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Choose a model supported by Groq. llama3-8b-8192 GroqÏóêÏÑú Í∞ÄÏû• Îπ†Î¶Ñ -> llama-3.1-8b-instant Î™®Îç∏Î°ú ÍµêÏ≤¥Îê® \n",
    "    temperature=0,  # Deterministic responses\n",
    "    max_tokens=512,\n",
    "    groq_api_key=os.environ[\"groq_api\"]\n",
    ")\n",
    "\n",
    "print(\"LLM initialized successfully\")\n",
    "\n",
    "# Test LLM\n",
    "test_response = llm.invoke(\"Hello, how are you?\")\n",
    "print(f\"LLM test response: {test_response.content[:100]}...\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e665e",
   "metadata": {},
   "source": [
    "### 8. Enhanced QA Chain with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20197290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:628: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.retrievers import BaseRetriever \n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class RetrievalWithReranking(BaseRetriever):\n",
    "    \"\"\"Custom retriever that combines vector search with reranking.\"\"\"\n",
    "    \n",
    "    vectorstore: any  # Your vectorstore\n",
    "    reranker: any     # Your reranker\n",
    "    k_initial: int = 10\n",
    "    k_final: int = 4\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def _get_relevant_documents(\n",
    "        self, \n",
    "        query: str, \n",
    "        *, \n",
    "        run_manager: CallbackManagerForRetrieverRun = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Retrieve and rerank documents.\"\"\"\n",
    "        # Step 1: Initial retrieval\n",
    "        initial_docs = self.vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=self.k_initial\n",
    "        )\n",
    "        \n",
    "        # Step 2: Rerank\n",
    "        if not initial_docs:\n",
    "            return []\n",
    "        \n",
    "        # Prepare documents for reranking\n",
    "        doc_texts = [doc.page_content for doc in initial_docs]\n",
    "        \n",
    "        # Get reranking scores\n",
    "        rerank_results = self.reranker.rerank(\n",
    "            query=query,\n",
    "            documents=doc_texts,\n",
    "            top_n=self.k_final\n",
    "        )\n",
    "        \n",
    "        # Reorder documents based on reranking scores\n",
    "        reranked_docs = []\n",
    "        for result in rerank_results:\n",
    "            reranked_docs.append(initial_docs[result['index']])\n",
    "        \n",
    "        return reranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cad29370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA chains created successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Enhanced prompt template\n",
    "template = \"\"\"Use the following context to answer the question accurately and concisely.\n",
    "If the answer is not in the context, say \"I cannot find this information in the provided context.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create enhanced retriever with reranking\n",
    "enhanced_retriever = RetrievalWithReranking(\n",
    "    vectorstore=vectorstore,\n",
    "    reranker=reranker,\n",
    "    k_initial=10,  # Retrieve more documents initially\n",
    "    k_final=4      # Use top 4 after reranking\n",
    ")\n",
    "\n",
    "# Create QA chain with reranking\n",
    "qa_chain_reranked = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=enhanced_retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Also create a regular chain for comparison\n",
    "regular_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain_regular = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=regular_retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"QA chains created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169263f1",
   "metadata": {},
   "source": [
    "### 9. Comparison: Regular vs Reranked RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7413e335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION: How much cash was used in operating activities in 2024?\n",
      "============================================================\n",
      "\n",
      "üîç REGULAR RAG:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/22wjv97x2yq_lzg77lp74ml40000gn/T/ipykernel_11148/2372916570.py:17: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  regular_result = qa_chain_regular({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: $115,877 million.\n",
      "Time: 0.605s\n",
      "Sources: 4 documents\n",
      "\n",
      "üîÑ RERANKED RAG:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DocumentReranker' object has no attribute 'rerank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Run comparisons\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m test_questions[:\u001b[38;5;241m2\u001b[39m]:  \u001b[38;5;66;03m# Test first 2 questions\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mcompare_rag_systems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 27\u001b[0m, in \u001b[0;36mcompare_rag_systems\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîÑ RERANKED RAG:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 27\u001b[0m reranked_result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain_reranked\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m reranked_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreranked_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:193\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     emit_warning()\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/langchain/chains/base.py:410\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    408\u001b[0m }\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/langchain/chains/base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    171\u001b[0m         inputs,\n\u001b[1;32m    172\u001b[0m         outputs,\n\u001b[1;32m    173\u001b[0m         return_only_outputs,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:156\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    152\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 156\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:278\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[0;34m(self, question, run_manager)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    273\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    275\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[1;32m    277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_llm/lib/python3.10/site-packages/langchain_core/retrievers.py:263\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m, in \u001b[0;36mRetrievalWithReranking._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     36\u001b[0m doc_texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m initial_docs]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Get reranking scores\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m rerank_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m(\n\u001b[1;32m     40\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m     41\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdoc_texts,\n\u001b[1;32m     42\u001b[0m     top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_final\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Reorder documents based on reranking scores\u001b[39;00m\n\u001b[1;32m     46\u001b[0m reranked_docs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DocumentReranker' object has no attribute 'rerank'"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"How much cash was used in operating activities in 2024?\",\n",
    "    \"Who is Brad D. Smith and what is his role?\",\n",
    "    \"What is Amazon's revenue for 2024?\",\n",
    "    \"What are the main business segments mentioned?\"\n",
    "]\n",
    "\n",
    "def compare_rag_systems(question):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Regular RAG\n",
    "    print(\"\\nüîç REGULAR RAG:\")\n",
    "    start_time = time.time()\n",
    "    regular_result = qa_chain_regular({\"query\": question})\n",
    "    regular_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Answer: {regular_result['result']}\")\n",
    "    print(f\"Time: {regular_time:.3f}s\")\n",
    "    print(f\"Sources: {len(regular_result['source_documents'])} documents\")\n",
    "    \n",
    "    # Reranked RAG\n",
    "    print(\"\\nüîÑ RERANKED RAG:\")\n",
    "    start_time = time.time()\n",
    "    reranked_result = qa_chain_reranked({\"query\": question})\n",
    "    reranked_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Answer: {reranked_result['result']}\")\n",
    "    print(f\"Time: {reranked_time:.3f}s\")\n",
    "    print(f\"Sources: {len(reranked_result['source_documents'])} documents\")\n",
    "    \n",
    "    # Show reranking scores if available\n",
    "    if reranked_result['source_documents']:\n",
    "        print(\"\\nReranking Details:\")\n",
    "        for i, doc in enumerate(reranked_result['source_documents']):\n",
    "            score = doc.metadata.get('rerank_score', 'N/A')\n",
    "            orig_pos = doc.metadata.get('original_position', 'N/A')\n",
    "            print(f\"  {i+1}. Score: {score:.3f}, Original position: {orig_pos}\")\n",
    "    \n",
    "    return regular_result, reranked_result\n",
    "\n",
    "# Run comparisons\n",
    "for question in test_questions[:2]:  # Test first 2 questions\n",
    "    compare_rag_systems(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef6784",
   "metadata": {},
   "source": [
    "### 10. Interactive RAG with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_rag_with_reranking():\n",
    "    \"\"\"Interactive RAG system with reranking.\"\"\"\n",
    "    print(\"üîÑ Interactive RAG with Reranking\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"‚ùì Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nü§ñ Processing...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = qa_chain_reranked({\"query\": question})\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\n‚úÖ Answer: {result['result']}\")\n",
    "            print(f\"‚è±Ô∏è  Response time: {response_time:.2f}s\")\n",
    "            \n",
    "            # Show source information\n",
    "            if result['source_documents']:\n",
    "                print(f\"\\nüìö Sources ({len(result['source_documents'])} documents):\")\n",
    "                for i, doc in enumerate(result['source_documents']):\n",
    "                    score = doc.metadata.get('rerank_score', 'N/A')\n",
    "                    page = doc.metadata.get('page', 'N/A')\n",
    "                    print(f\"  {i+1}. Page {page}, Rerank Score: {score:.3f}\")\n",
    "                    print(f\"     {doc.page_content[:100]}...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_rag_with_reranking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a8b4a",
   "metadata": {},
   "source": [
    "### 11. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18700ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_reranking_performance(questions):\n",
    "    \"\"\"Analyze performance differences between regular and reranked RAG.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Testing: {question[:50]}...\")\n",
    "        \n",
    "        # Regular RAG\n",
    "        start_time = time.time()\n",
    "        regular_result = qa_chain_regular({\"query\": question})\n",
    "        regular_time = time.time() - start_time\n",
    "        \n",
    "        # Reranked RAG\n",
    "        start_time = time.time()\n",
    "        reranked_result = qa_chain_reranked({\"query\": question})\n",
    "        reranked_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'question': question[:30] + '...',\n",
    "            'regular_time': regular_time,\n",
    "            'reranked_time': reranked_time,\n",
    "            'time_overhead': reranked_time - regular_time,\n",
    "            'regular_answer_length': len(regular_result['result']),\n",
    "            'reranked_answer_length': len(reranked_result['result'])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze performance\n",
    "if len(test_questions) > 0:\n",
    "    print(\"Analyzing performance...\")\n",
    "    perf_df = analyze_reranking_performance(test_questions)\n",
    "    \n",
    "    print(\"\\nüìä Performance Analysis:\")\n",
    "    print(perf_df[['question', 'regular_time', 'reranked_time', 'time_overhead']].round(3))\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_regular_time = perf_df['regular_time'].mean()\n",
    "    avg_reranked_time = perf_df['reranked_time'].mean()\n",
    "    avg_overhead = perf_df['time_overhead'].mean()\n",
    "    \n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"   Average Regular Time: {avg_regular_time:.3f}s\")\n",
    "    print(f\"   Average Reranked Time: {avg_reranked_time:.3f}s\")\n",
    "    print(f\"   Average Overhead: {avg_overhead:.3f}s ({(avg_overhead/avg_regular_time)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec876994",
   "metadata": {},
   "source": [
    "### 12. Summary and Best Practices\n",
    "\n",
    "#### What We Implemented:\n",
    "1. **Two-Stage Retrieval**: Initial retrieval + Cross-encoder reranking\n",
    "2. **Multiple Reranker Models**: From fast TinyBERT to accurate MiniLM-L12\n",
    "3. **Performance Optimization**: Batch processing and caching\n",
    "4. **Comprehensive Comparison**: Regular vs Reranked RAG\n",
    "\n",
    "#### Key Benefits of Reranking:\n",
    "- üéØ **Better Accuracy**: Cross-encoders understand query-document relationships better\n",
    "- üîç **Context Awareness**: Models can evaluate semantic relevance more precisely\n",
    "- ‚ö° **Efficiency**: Only rerank top candidates, not entire corpus\n",
    "- üìä **Measurable Improvement**: Clear ranking scores for transparency\n",
    "\n",
    "#### Best Practices:\n",
    "1. **Model Selection**: Balance speed vs accuracy based on your needs\n",
    "2. **Threshold Tuning**: Filter low-relevance documents\n",
    "3. **Batch Size**: Optimize for your hardware\n",
    "4. **Initial Retrieval**: Retrieve more documents (10-20) for better reranking candidates\n",
    "5. **Final Selection**: Keep 3-5 top documents for LLM processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fbd44",
   "metadata": {},
   "source": [
    "### 13. Export Enhanced RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e764e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_rag_with_reranking(query, verbose=True):\n",
    "    \"\"\"Enhanced RAG function with reranking.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = qa_chain_reranked({\"query\": query})\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Question: {query}\")\n",
    "            print(f\"Answer: {result['result']}\")\n",
    "            print(f\"Response time: {response_time:.3f}s\")\n",
    "            \n",
    "            if result['source_documents']:\n",
    "                print(f\"\\nTop sources:\")\n",
    "                for i, doc in enumerate(result['source_documents'][:2]):\n",
    "                    score = doc.metadata.get('rerank_score', 'N/A')\n",
    "                    print(f\"  {i+1}. Score: {score:.3f} - {doc.page_content[:100]}...\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the enhanced function\n",
    "print(\"Testing enhanced RAG function:\")\n",
    "enhanced_rag_with_reranking(\"What was Amazon's operating cash flow in 2024?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
