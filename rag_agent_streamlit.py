import streamlit as st
import os
import tempfile
import time
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import pickle
from pathlib import Path

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG AI-Agent (Optimized)",
    page_icon="âš¡",
    layout="wide"
)

# ì œëª©ê³¼ ì„¤ëª…
st.title("âš¡ RAG AI-Agent (ìµœì í™” ë²„ì „)")
st.markdown("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
CACHE_DIR = Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

# ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ ë° ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    
    # Provider ì„ íƒ
    provider = st.selectbox(
        "AI Provider ì„ íƒ",
        ["Groq", "Anthropic", "OpenAI"],
        help="ì‚¬ìš©í•  AI ëª¨ë¸ ì œê³µì—…ì²´ë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )
    
    # Providerë³„ API í‚¤ ì…ë ¥
    if provider == "Groq":
        api_key = st.text_input(
            "Groq API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            type="password",
            help="Groq API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )
    elif provider == "Anthropic":
        api_key = st.text_input(
            "Anthropic API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            type="password",
            help="Anthropic API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )
    elif provider == "OpenAI":
        api_key = st.text_input(
            "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            type="password",
            help="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )
    
    st.subheader("ì„±ëŠ¥ ì„¤ì •")
    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 500, 2000, 1000, 100)
    chunk_overlap = st.slider("ì²­í¬ ê²¹ì¹¨", 50, 300, 200, 50)
    k_retrieval = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜", 2, 10, 3)
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    embedding_model = st.selectbox(
        "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
        [
            "sentence-transformers/all-MiniLM-L6-v2",  # ê°€ì¥ ë¹ ë¦„
            "sentence-transformers/all-mpnet-base-v2",  # ê· í˜•
            "BAAI/bge-small-en-v1.5",  # ì‘ê³  ë¹ ë¦„
        ],
        index=0
    )
    
    # Providerë³„ LLM ëª¨ë¸ ì„ íƒ
    if provider == "Groq":
        llm_model = st.selectbox(
            "Groq LLM ëª¨ë¸ ì„ íƒ",
            [
                "llama3-8b-8192",     # ê°€ì¥ ë¹ ë¦„
                "llama3-70b-8192",    # ì„±ëŠ¥ ì¢‹ìŒ
                "mixtral-8x7b-32768", # ê¸´ ì»¨í…ìŠ¤íŠ¸
            ],
            index=0
        )
    elif provider == "Anthropic":
        llm_model = st.selectbox(
            "Anthropic LLM ëª¨ë¸ ì„ íƒ",
            [
                "claude-3-5-sonnet-20241022",  # ìµœì‹  Sonnet
                "claude-3-5-haiku-20241022",   # ë¹ ë¥¸ Haiku
                "claude-3-opus-20240229",      # ê°•ë ¥í•œ Opus
            ],
            index=0
        )
    elif provider == "OpenAI":
        llm_model = st.selectbox(
            "OpenAI LLM ëª¨ë¸ ì„ íƒ",
            [
                "gpt-4o",           # ìµœì‹  GPT-4o
                "gpt-4o-mini",      # ë¹ ë¥¸ GPT-4o mini
                "gpt-3.5-turbo",    # ë¹ ë¥¸ GPT-3.5
            ],
            index=0
        )
    
    # ìºì‹œ ê´€ë¦¬
    if st.button("ìºì‹œ ì´ˆê¸°í™”"):
        for cache_file in CACHE_DIR.glob("*.pkl"):
            cache_file.unlink()
        st.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # API í‚¤ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    if api_key:
        if provider == "Groq":
            os.environ["GROQ_API_KEY"] = api_key
        elif provider == "Anthropic":
            os.environ["ANTHROPIC_API_KEY"] = api_key
        elif provider == "OpenAI":
            os.environ["OPENAI_API_KEY"] = api_key

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'qa_chain' not in st.session_state:
    st.session_state.qa_chain = None
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None

@st.cache_resource
def load_embeddings(model_name: str):
    """ì„ë² ë”© ëª¨ë¸ì„ ìºì‹œí•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
            encode_kwargs={'normalize_embeddings': True}  # ì„±ëŠ¥ í–¥ìƒ
        )
        return embeddings
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {str(e)}")
        return None

@st.cache_data
def load_pdf_cached(pdf_bytes, filename):
    """PDF ë¡œë”©ì„ ìºì‹œí•©ë‹ˆë‹¤."""
    cache_file = CACHE_DIR / f"{filename}_{hash(pdf_bytes)}.pkl"
    
    if cache_file.exists():
        st.info("ìºì‹œëœ PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    
    # ìƒˆë¡œìš´ PDF ì²˜ë¦¬
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_bytes)
        tmp_file_path = tmp_file.name
    
    try:
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        
        # ìºì‹œ ì €ì¥
        with open(cache_file, 'wb') as f:
            pickle.dump(documents, f)
        
        return documents
    finally:
        os.unlink(tmp_file_path)

def create_vectorstore_optimized(documents: List, embeddings, chunk_size: int, chunk_overlap: int):
    """ìµœì í™”ëœ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    try:
        if not documents:
            return None, 0
        
        # ë¬¸ì„œ í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±
        doc_hash = hash(str([doc.page_content for doc in documents]))
        cache_file = CACHE_DIR / f"vectorstore_{doc_hash}_{chunk_size}_{chunk_overlap}.pkl"
        
        if cache_file.exists():
            st.info("ìºì‹œëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            with open(cache_file, 'rb') as f:
                vectorstore = pickle.load(f)
                return vectorstore, len(documents)
        
        # ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len
        )
        
        texts = text_splitter.split_documents(documents)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 32
        vectorstore = None
        
        progress_bar = st.progress(0)
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            if vectorstore is None:
                vectorstore = FAISS.from_documents(batch, embeddings)
            else:
                temp_vs = FAISS.from_documents(batch, embeddings)
                vectorstore.merge_from(temp_vs)
            
            progress = (i + batch_size) / len(texts)
            progress_bar.progress(min(progress, 1.0))
        
        progress_bar.empty()
        
        # ìºì‹œ ì €ì¥
        with open(cache_file, 'wb') as f:
            pickle.dump(vectorstore, f)
        
        return vectorstore, len(texts)
        
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, 0

def create_qa_chain_optimized(vectorstore, api_key: str, model_name: str, k: int, provider: str):
    """ìµœì í™”ëœ QA ì²´ì¸ ìƒì„±"""
    try:
        # Providerë³„ LLM ì„¤ì •
        if provider == "Groq":
            llm = ChatGroq(
                temperature=0,
                model_name=model_name,
                groq_api_key=api_key,
                max_tokens=1024,  # ì‘ë‹µ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì†ë„ í–¥ìƒ
            )
        elif provider == "Anthropic":
            llm = ChatAnthropic(
                model=model_name,
                temperature=0,
                max_tokens=1024,
                api_key=api_key,
            )
        elif provider == "OpenAI":
            llm = ChatOpenAI(
                model=model_name,
                temperature=0,
                max_tokens=1024,
                api_key=api_key,
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}")
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (í† í° ìˆ˜ ìµœì†Œí™”)
        prompt_template = """Context: {context}

Question: {question}
Answer:"""

        PROMPT = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"), ])
        
        # ê²€ìƒ‰ê¸° ì„¤ì • ìµœì í™”
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": k,
                "fetch_k": k * 2  # ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•œ ì˜¤ë²„í˜ì¹­
            }
        )
        
        # qa_chain = RetrievalQA.from_chain_type(
        #     llm=llm,
        #     chain_type="stuff",
        #     retriever=retriever,
        #     chain_type_kwargs={"prompt": PROMPT},
        #     return_source_documents=True
        # )
        # return qa_chain

        # Create the chains 
        question_answer_chain = create_stuff_documents_chain(llm, PROMPT)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)
        return rag_chain
    except Exception as e:
        st.error(f"QA ì²´ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ë©”ì¸ ë ˆì´ì•„ì›ƒ
col1, col2 = st.columns([1, 2])

with col1:
    st.header("ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type="pdf",
        help="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    if uploaded_file and api_key:
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        if st.session_state.embeddings is None:
            with st.spinner("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                st.session_state.embeddings = load_embeddings(embedding_model)
        
        if st.session_state.embeddings:
            with st.spinner("PDFë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                start_time = time.time()
                
                # PDF ë¡œë“œ (ìºì‹œë¨)
                pdf_bytes = uploaded_file.read()
                documents = load_pdf_cached(pdf_bytes, uploaded_file.name)
                
                if documents:
                    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ìºì‹œë¨)
                    vectorstore, chunk_count = create_vectorstore_optimized(
                        documents, 
                        st.session_state.embeddings, 
                        chunk_size, 
                        chunk_overlap
                    )
                    
                    if vectorstore:
                        # QA ì²´ì¸ ìƒì„±
                        qa_chain = create_qa_chain_optimized(
                            vectorstore, 
                            api_key, 
                            llm_model, 
                            k_retrieval,
                            provider
                        )
                        
                        if qa_chain:
                            st.session_state.vectorstore = vectorstore
                            st.session_state.qa_chain = qa_chain
                            
                            processing_time = time.time() - start_time
                            st.success(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ! ({processing_time:.1f}ì´ˆ)")
                            
                            # ìµœì í™”ëœ ë¬¸ì„œ ì •ë³´
                            total_text = sum(len(doc.page_content) for doc in documents)
                            st.info(f"""ğŸ“Š ë¬¸ì„œ ì •ë³´
- íŒŒì¼ëª…: {uploaded_file.name}
- í˜ì´ì§€ ìˆ˜: {len(documents)}
- ì²­í¬ ìˆ˜: {chunk_count}
- í…ìŠ¤íŠ¸: {total_text:,} ë¬¸ì
- ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ""")
                else:
                    st.error("PDF ë¡œë”© ì‹¤íŒ¨")
    
    elif uploaded_file and not api_key:
        st.warning(f"âš ï¸ {provider} API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # ìƒíƒœ í‘œì‹œ
    if st.session_state.qa_chain:
        st.success("ğŸ¤– AI Agent ì¤€ë¹„ ì™„ë£Œ!")
    else:
        st.info("ğŸ“‹ PDFë¥¼ ì—…ë¡œë“œí•˜ê³  API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

with col2:
    st.header("ğŸ’¬ ì§ˆë¬¸ & ë‹µë³€")
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.chat_message("user").write(message["content"])
        else:
            st.chat_message("assistant").write(message["content"])
    
    # ì§ˆë¬¸ ì…ë ¥ (API í‚¤ê°€ ìœ íš¨í•  ë•Œë§Œ)
    if st.session_state.qa_chain and api_key:
        if prompt := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            
            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                start_time = time.time()
                
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        result = st.session_state.qa_chain.invoke({"input": prompt})
                        answer = result.get("answer", "")
                        
                        response_time = time.time() - start_time
                        
                        st.write(answer)
                        st.caption(f"ì‘ë‹µ ì‹œê°„: {response_time:.1f}ì´ˆ")
                        
                        # ì†ŒìŠ¤ ë¬¸ì„œ (ì¶•ì•½ëœ ì •ë³´)
                        context_docs = result.get("context", [])
                        if context_docs:
                            with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ"):
                                for i, doc in enumerate(context_docs[:2]):
                                    page_info = ""
                                    if hasattr(doc, 'metadata') and 'page' in doc.metadata:
                                        page_info = f" (p.{doc.metadata['page'] + 1})"
                                    
                                    st.write(f"**ì°¸ê³  {i+1}{page_info}:**")
                                    st.write(doc.page_content[:200] + "...")
                        
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                        
                    except Exception as e:
                        error_msg = str(e)
                        
                        # Providerë³„ API ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
                        if "Error code: 401" in error_msg or "authentication_error" in error_msg or "invalid x-api-key" in error_msg:
                            error_display = f"ğŸ”‘ **API í‚¤ ì¸ì¦ ì‹¤íŒ¨**: {provider} API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì •í™•í•œ API í‚¤ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."
                        elif "Error code: 429" in error_msg:
                            error_display = f"â° **ì‚¬ìš©ëŸ‰ í•œë„ ì´ˆê³¼**: {provider} API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        elif "Error code: 500" in error_msg:
                            error_display = f"ğŸ”§ **ì„œë²„ ì˜¤ë¥˜**: {provider} ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        elif "anthropic" in error_msg.lower() and ("api_key" in error_msg.lower() or "authentication" in error_msg.lower()):
                            error_display = f"ğŸ”‘ **API í‚¤ ì¸ì¦ ì‹¤íŒ¨**: {provider} API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì •í™•í•œ API í‚¤ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."
                        elif "openai" in error_msg.lower() and ("api_key" in error_msg.lower() or "authentication" in error_msg.lower()):
                            error_display = f"ğŸ”‘ **API í‚¤ ì¸ì¦ ì‹¤íŒ¨**: {provider} API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì •í™•í•œ API í‚¤ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."
                        elif "groq" in error_msg.lower() and ("api_key" in error_msg.lower() or "authentication" in error_msg.lower()):
                            error_display = f"ğŸ”‘ **API í‚¤ ì¸ì¦ ì‹¤íŒ¨**: {provider} API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì •í™•í•œ API í‚¤ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."
                        else:
                            error_display = f"âŒ **ì˜¤ë¥˜ ë°œìƒ**: {error_msg}"
                        
                        st.error(error_display)
                        st.session_state.messages.append({"role": "assistant", "content": error_display})
    elif not api_key:
        st.info(f"ë¨¼ì € {provider} API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.info("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if st.session_state.messages:
        if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸°"):
            st.session_state.messages = []
            st.rerun()

# ì„±ëŠ¥ íŒ
st.markdown("---")
with st.expander("âš¡ ì„±ëŠ¥ ìµœì í™” íŒ"):
    st.markdown("""
    ### ğŸš€ ì†ë„ í–¥ìƒ ë°©ë²•:
    1. **ì„ë² ë”© ëª¨ë¸**: `all-MiniLM-L6-v2` ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
    2. **LLM ëª¨ë¸**: 
       - **Groq**: `llama3-8b-8192` (ê°€ì¥ ë¹ ë¦„)
       - **Anthropic**: `claude-3-5-haiku-20241022` (ë¹ ë¦„)
       - **OpenAI**: `gpt-4o-mini` (ë¹ ë¦„)
    3. **ì²­í¬ í¬ê¸°**: í° ê°’(1500-2000)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì²­í¬ ìˆ˜ ê°ì†Œ
    4. **ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜**: 2-3ê°œë¡œ ì œí•œ
    5. **ìºì‹œ í™œìš©**: ë™ì¼í•œ PDFëŠ” ìºì‹œì—ì„œ ë¡œë“œ
    
    ### ğŸ¯ í˜„ì¬ ìµœì í™” ì ìš© ì‚¬í•­:
    - âœ… ì„ë² ë”© ëª¨ë¸ ìºì‹± (`@st.cache_resource`)
    - âœ… PDF ì²˜ë¦¬ ê²°ê³¼ ìºì‹± (`@st.cache_data`)
    - âœ… ë²¡í„°ìŠ¤í† ì–´ ë””ìŠ¤í¬ ìºì‹±
    - âœ… ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬
    - âœ… ë‹¤ì¤‘ Provider ì§€ì› (Groq, Anthropic, OpenAI)
    - âœ… ì‘ë‹µ í† í° ìˆ˜ ì œí•œ
    """)

st.code("streamlit run app.py", language="bash")