{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb1c8c9",
   "metadata": {},
   "source": [
    "### `Llama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8064188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_call_id=None, tool_calls=[])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "os.chdir('/Users/reejungkim/Documents/Git/working-in-progress')\n",
    "load_dotenv()\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=os.environ[\"huggingface_read\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cb37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2f0a6",
   "metadata": {},
   "source": [
    "### `Groq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a63edb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"groq\",\n",
    "    api_key=os.environ[\"huggingface_read\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9910144",
   "metadata": {},
   "source": [
    "### `GPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf5840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir('/Users/reejungkim/Documents/Git/working-in-progress')\n",
    "load_dotenv()\n",
    "\n",
    "def parse_response(response_str):\n",
    "    data = json.loads(response_str)\n",
    "    return data[\"response\"].replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "def get_response(prompt):\n",
    "    client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"huggingface_read\"],\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openai/gpt-oss-20b:novita\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt \n",
    "            }\n",
    "        ],\n",
    "        #response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return print(completion.choices[0].message.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf22534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a catalog of the most widelyâ€‘used **openâ€‘source Large Language Models (LLMs)** that are hosted on the Huggingâ€¯Face Model Hub.  \n",
      "The list is organized by family, with *model name*, *parameter count*, *license*, *primary useâ€‘case* and a quick link to the Hub page.  \n",
      "(â€œOpenâ€‘sourceâ€ here means that the weights are available under a permissive license that lets you fineâ€‘tune, serve, or modify the model without a restrictive commercial gateâ€‘keeping policy.)\n",
      "\n",
      "> **Important note** â€“ Always doubleâ€‘check the specific license in the model card before deploying to production. Some models (e.g. LLaMA 2) are researchâ€‘only until you obtain a commercial license from Meta.\n",
      "\n",
      "| Family | Model | Params | License (if any) | Typical Useâ€‘Cases | Hub URL |\n",
      "|--------|-------|--------|------------------|-------------------|---------|\n",
      "| **Meta LLaMA** | LLaMAâ€‘2â€‘7B | 7â€¯B | Llama 2 LICENSE â€“ *research only by default* | Text completion, chat, summarisation | <https://huggingface.co/meta-llama/Meta-Llama-2-7B> |\n",
      "| | LLaMAâ€‘2â€‘13B | 13â€¯B | Same | Same | <https://huggingface.co/meta-llama/Meta-Llama-2-13B> |\n",
      "| | LLaMAâ€‘2â€‘70B | 70â€¯B | Same | Same | <https://huggingface.co/meta-llama/Meta-Llama-2-70B> |\n",
      "| **Mistral** | Mistralâ€‘7B | 7â€¯B | CCâ€‘BYâ€‘4.0 | Generation, chat, coding assistant | <https://huggingface.co/mistralai/Mistral-7B-v0.1> |\n",
      "| | Mistralâ€‘7B-Instruct | 7â€¯B | CCâ€‘BYâ€‘4.0 | Instructionâ€‘fineâ€‘tuned | <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1> |\n",
      "| **Mixtral** | Mixtralâ€‘8x7B | 61â€¯B | CCâ€‘BYâ€‘4.0 | Highâ€‘capacity inference, multimodal (via decoders) | <https://huggingface.co/mixtral/mixtral-8x7B-v0.1> |\n",
      "| **Falcon** | Falcon-7B | 7â€¯B | Apacheâ€‘2.0 | Text generation, summarisation, chat (via Llamaâ€‘Style adapters) | <https://huggingface.co/tiiuae/falcon-7b> |\n",
      "| | Falcon-40B | 40â€¯B | Apacheâ€‘2.0 | Same | <https://huggingface.co/tiiuae/falcon-40B> |\n",
      "| **Openâ€‘Chat** | OpenChat_3.5 | 3.5â€¯B | CCâ€‘BYâ€‘4.0 | Finetuning for chat, lightâ€‘weight | <https://huggingface.co/memray/openchat-3.5> |\n",
      "| **Polyglot** | ggml-alpaca-lora-7b | 7â€¯B | MIT | Lowâ€‘resource serving (LLAMAâ€‘2 + LoRA) | <https://huggingface.co/thebloke/ggml-alpaca-lora-7b> |\n",
      "| **Phi** | Phiâ€‘2 | 2â€¯B | MIT | Conversational, lowâ€‘latency on edge devices | <https://huggingface.co/microsoft/phi-2> |\n",
      "| | Phiâ€‘1.5 | 1.5â€¯B | MIT | Lightâ€‘weight inference, student models | <https://huggingface.co/microsoft/phi-1.5> |\n",
      "| **BLOOM** | BLOOM | 176â€¯B | Apacheâ€‘2.0 (plus Human Rights license) | Multilingual generation, researchâ€‘grade | <https://huggingface.co/bigscience/bloom> |\n",
      "| | BLOOMâ€‘Z | 176â€¯B | Same | Democratised fineâ€‘tuning, inâ€‘house training workflow | <https://huggingface.co/bigscience/bloomz> |\n",
      "| **Openâ€‘AI GPTâ€‘Neo** | GPTâ€‘Neoâ€‘2.7B | 2.7â€¯B | MIT | GPTâ€‘2 style generation, openâ€‘model research | <https://huggingface.co/EleutherAI/gpt-neo-2.7B> |\n",
      "| | GPTâ€‘NeoX-20B | 20â€¯B | MIT | Faster training, larger capacity, openâ€‘model | <https://huggingface.co/EleutherAI/gpt-neox-20b> |\n",
      "| **Qwen** | Qwenâ€‘7Bâ€‘Chat | 7â€¯B | Apacheâ€‘2.0 | Chat, domainâ€‘specific fineâ€‘tuning | <https://huggingface.co/Qwen/Qwen-7B-Chat> |\n",
      "| | Qwenâ€‘14Bâ€‘Chat | 14â€¯B | Apacheâ€‘2.0 | Heavyâ€‘weight chat, larger context windows | <https://huggingface.co/Qwen/Qwen-14B-Chat> |\n",
      "| **Stable Diffusion 3â€‘LM** | SD3â€‘Tinyâ€‘93B | 93â€¯B (textâ€‘only portion) | CCâ€‘BYâ€‘4.0 (for the LLM part) | Multimodal generation (text + image) | <https://huggingface.co/stabilityai/stabilityai-sd3-tiny> |\n",
      "| **Openâ€‘AI GPTâ€‘4o Mini (beta)** | GPTâ€‘4oâ€‘Mini | 13â€¯B | Apacheâ€‘2.0 | Very lightweight for onâ€‘device chat | <https://huggingface.co/openai/chatgpt-4o-mini> |\n",
      "| **General Assistants** | Llamaâ€‘Guard | 7â€¯B | MIT | Guardrails for LLM safety (instructionâ€‘based filtering) | <https://huggingface.co/chavinlo/LlamaGuard-7B> |\n",
      "| | CodeLlamaâ€‘7B | 7â€¯B | Apacheâ€‘2.0 | Code generation, instruction capitalisation | <https://huggingface.co/codellama/CodeLlama-7b-Instruct> |\n",
      "\n",
      "### How to pick one\n",
      "\n",
      "| Need | Recommended family | Why |\n",
      "|------|-------------------|-----|\n",
      "| **General text generation** | Falconâ€‘7B, Mistralâ€‘7B, LLaMAâ€‘2â€‘7B | Balanced size/latency, fast inference on consumer GPUs |\n",
      "| **Largeâ€‘scale research** | Mixtralâ€‘8x7B, LLaMAâ€‘2â€‘70B, BLOOM | Highest capacity & multilingual support |\n",
      "| **Chatâ€‘specific fineâ€‘tuning** | Mistralâ€‘7Bâ€‘Instruct, Qwenâ€‘13Bâ€‘Chat, LLaMAâ€‘2â€‘13B (with fineâ€‘tune) | Models come with chatâ€‘aligned prompt templates |\n",
      "| **Very small footprint** | Phiâ€‘1.5, Llamaâ€‘Guard (7B) | 1â€“2â€¯B param models are good for edge or 8â€‘bit quantized deployment |\n",
      "| **Multimodal (text + image)** | Stable Diffusion 3â€‘LM, LLaMAâ€‘2â€‘13B (with image encoder) | For VQA, captioning, or imageâ€‘toâ€‘text pipelines |\n",
      "\n",
      "### Quick sideâ€‘note on licenses\n",
      "\n",
      "- **Meta LLaMAâ€‘2**: For research use only. You must purchase a commercial license from Meta if you plan to use the models in a commercial product.\n",
      "- **Mistral** and **Mixtral**: CCâ€‘BYâ€‘4.0 â€“ can be used for commercial deployments.\n",
      "- **Falcon**, **Phi**, **Qwen**: Apacheâ€‘2.0 â€“ extremely permissive.\n",
      "- **BLOOM**: Apacheâ€‘2.0 *plus* a humanâ€‘rightsâ€‘respecting clause that limits exploitative uses.\n",
      "\n",
      "---\n",
      "\n",
      "**Pro tip** â€“ for any production ingestion, consider using Huggingâ€¯Faceâ€™s *AutoModelForCausalLM* abstraction together with *transformers.quantization_utils* to convert the models to 4â€‘bit (or 8â€‘bit) dynamic quantization, cutting GPU memory 4Ã— while preserving nearâ€‘original quality. \n",
      "\n",
      "Feel free to let me know if youâ€™d like deeper details on a particular model (API usage, training instructions, or benchmark comparison).\n"
     ]
    }
   ],
   "source": [
    "get_response(\"list open sourced llm on huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9a5bd",
   "metadata": {},
   "source": [
    "## InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b3dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! ðŸ˜Š How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(token=os.environ[\"huggingface_read\"])\n",
    "response = client.chat_completion(\n",
    "    model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "#print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7db4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
