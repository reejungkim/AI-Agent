{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb1c8c9",
   "metadata": {},
   "source": [
    "### `Llama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8064188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir('/Users/reejungkim/Documents/Git/working-in-progress')\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=os.environ[\"huggingface_read\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cb37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2f0a6",
   "metadata": {},
   "source": [
    "### `Groq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a63edb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"groq\",\n",
    "    api_key=os.environ[\"huggingface_read\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdb0f7",
   "metadata": {},
   "source": [
    "`Groq as a provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1feb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=8192,\n",
    "    top_p=1,\n",
    "    reasoning_effort=\"medium\",\n",
    "    stream=True,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9910144",
   "metadata": {},
   "source": [
    "### `GPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir('/Users/reejungkim/Documents/Git/working-in-progress')\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"openai_api\"])   \n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"Talk like a pirate.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do I check if a Python object is an instance of a class?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf5840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir('/Users/reejungkim/Documents/Git/working-in-progress')\n",
    "load_dotenv()\n",
    "\n",
    "def parse_response(response_str):\n",
    "    data = json.loads(response_str)\n",
    "    return data[\"response\"].replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "def get_response(prompt):\n",
    "    client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"huggingface_read\"],\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openai/gpt-oss-20b:novita\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt \n",
    "            }\n",
    "        ],\n",
    "        #response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return print(completion.choices[0].message.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf22534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a catalog of the most widely‑used **open‑source Large Language Models (LLMs)** that are hosted on the Hugging Face Model Hub.  \n",
      "The list is organized by family, with *model name*, *parameter count*, *license*, *primary use‑case* and a quick link to the Hub page.  \n",
      "(“Open‑source” here means that the weights are available under a permissive license that lets you fine‑tune, serve, or modify the model without a restrictive commercial gate‑keeping policy.)\n",
      "\n",
      "> **Important note** – Always double‑check the specific license in the model card before deploying to production. Some models (e.g. LLaMA 2) are research‑only until you obtain a commercial license from Meta.\n",
      "\n",
      "| Family | Model | Params | License (if any) | Typical Use‑Cases | Hub URL |\n",
      "|--------|-------|--------|------------------|-------------------|---------|\n",
      "| **Meta LLaMA** | LLaMA‑2‑7B | 7 B | Llama 2 LICENSE – *research only by default* | Text completion, chat, summarisation | <https://huggingface.co/meta-llama/Meta-Llama-2-7B> |\n",
      "| | LLaMA‑2‑13B | 13 B | Same | Same | <https://huggingface.co/meta-llama/Meta-Llama-2-13B> |\n",
      "| | LLaMA‑2‑70B | 70 B | Same | Same | <https://huggingface.co/meta-llama/Meta-Llama-2-70B> |\n",
      "| **Mistral** | Mistral‑7B | 7 B | CC‑BY‑4.0 | Generation, chat, coding assistant | <https://huggingface.co/mistralai/Mistral-7B-v0.1> |\n",
      "| | Mistral‑7B-Instruct | 7 B | CC‑BY‑4.0 | Instruction‑fine‑tuned | <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1> |\n",
      "| **Mixtral** | Mixtral‑8x7B | 61 B | CC‑BY‑4.0 | High‑capacity inference, multimodal (via decoders) | <https://huggingface.co/mixtral/mixtral-8x7B-v0.1> |\n",
      "| **Falcon** | Falcon-7B | 7 B | Apache‑2.0 | Text generation, summarisation, chat (via Llama‑Style adapters) | <https://huggingface.co/tiiuae/falcon-7b> |\n",
      "| | Falcon-40B | 40 B | Apache‑2.0 | Same | <https://huggingface.co/tiiuae/falcon-40B> |\n",
      "| **Open‑Chat** | OpenChat_3.5 | 3.5 B | CC‑BY‑4.0 | Finetuning for chat, light‑weight | <https://huggingface.co/memray/openchat-3.5> |\n",
      "| **Polyglot** | ggml-alpaca-lora-7b | 7 B | MIT | Low‑resource serving (LLAMA‑2 + LoRA) | <https://huggingface.co/thebloke/ggml-alpaca-lora-7b> |\n",
      "| **Phi** | Phi‑2 | 2 B | MIT | Conversational, low‑latency on edge devices | <https://huggingface.co/microsoft/phi-2> |\n",
      "| | Phi‑1.5 | 1.5 B | MIT | Light‑weight inference, student models | <https://huggingface.co/microsoft/phi-1.5> |\n",
      "| **BLOOM** | BLOOM | 176 B | Apache‑2.0 (plus Human Rights license) | Multilingual generation, research‑grade | <https://huggingface.co/bigscience/bloom> |\n",
      "| | BLOOM‑Z | 176 B | Same | Democratised fine‑tuning, in‑house training workflow | <https://huggingface.co/bigscience/bloomz> |\n",
      "| **Open‑AI GPT‑Neo** | GPT‑Neo‑2.7B | 2.7 B | MIT | GPT‑2 style generation, open‑model research | <https://huggingface.co/EleutherAI/gpt-neo-2.7B> |\n",
      "| | GPT‑NeoX-20B | 20 B | MIT | Faster training, larger capacity, open‑model | <https://huggingface.co/EleutherAI/gpt-neox-20b> |\n",
      "| **Qwen** | Qwen‑7B‑Chat | 7 B | Apache‑2.0 | Chat, domain‑specific fine‑tuning | <https://huggingface.co/Qwen/Qwen-7B-Chat> |\n",
      "| | Qwen‑14B‑Chat | 14 B | Apache‑2.0 | Heavy‑weight chat, larger context windows | <https://huggingface.co/Qwen/Qwen-14B-Chat> |\n",
      "| **Stable Diffusion 3‑LM** | SD3‑Tiny‑93B | 93 B (text‑only portion) | CC‑BY‑4.0 (for the LLM part) | Multimodal generation (text + image) | <https://huggingface.co/stabilityai/stabilityai-sd3-tiny> |\n",
      "| **Open‑AI GPT‑4o Mini (beta)** | GPT‑4o‑Mini | 13 B | Apache‑2.0 | Very lightweight for on‑device chat | <https://huggingface.co/openai/chatgpt-4o-mini> |\n",
      "| **General Assistants** | Llama‑Guard | 7 B | MIT | Guardrails for LLM safety (instruction‑based filtering) | <https://huggingface.co/chavinlo/LlamaGuard-7B> |\n",
      "| | CodeLlama‑7B | 7 B | Apache‑2.0 | Code generation, instruction capitalisation | <https://huggingface.co/codellama/CodeLlama-7b-Instruct> |\n",
      "\n",
      "### How to pick one\n",
      "\n",
      "| Need | Recommended family | Why |\n",
      "|------|-------------------|-----|\n",
      "| **General text generation** | Falcon‑7B, Mistral‑7B, LLaMA‑2‑7B | Balanced size/latency, fast inference on consumer GPUs |\n",
      "| **Large‑scale research** | Mixtral‑8x7B, LLaMA‑2‑70B, BLOOM | Highest capacity & multilingual support |\n",
      "| **Chat‑specific fine‑tuning** | Mistral‑7B‑Instruct, Qwen‑13B‑Chat, LLaMA‑2‑13B (with fine‑tune) | Models come with chat‑aligned prompt templates |\n",
      "| **Very small footprint** | Phi‑1.5, Llama‑Guard (7B) | 1–2 B param models are good for edge or 8‑bit quantized deployment |\n",
      "| **Multimodal (text + image)** | Stable Diffusion 3‑LM, LLaMA‑2‑13B (with image encoder) | For VQA, captioning, or image‑to‑text pipelines |\n",
      "\n",
      "### Quick side‑note on licenses\n",
      "\n",
      "- **Meta LLaMA‑2**: For research use only. You must purchase a commercial license from Meta if you plan to use the models in a commercial product.\n",
      "- **Mistral** and **Mixtral**: CC‑BY‑4.0 – can be used for commercial deployments.\n",
      "- **Falcon**, **Phi**, **Qwen**: Apache‑2.0 – extremely permissive.\n",
      "- **BLOOM**: Apache‑2.0 *plus* a human‑rights‑respecting clause that limits exploitative uses.\n",
      "\n",
      "---\n",
      "\n",
      "**Pro tip** – for any production ingestion, consider using Hugging Face’s *AutoModelForCausalLM* abstraction together with *transformers.quantization_utils* to convert the models to 4‑bit (or 8‑bit) dynamic quantization, cutting GPU memory 4× while preserving near‑original quality. \n",
      "\n",
      "Feel free to let me know if you’d like deeper details on a particular model (API usage, training instructions, or benchmark comparison).\n"
     ]
    }
   ],
   "source": [
    "get_response(\"list open sourced llm on huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9a5bd",
   "metadata": {},
   "source": [
    "## InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b3dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! 😊 How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(token=os.environ[\"huggingface_read\"])\n",
    "response = client.chat_completion(\n",
    "    model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "#print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e562f6b",
   "metadata": {},
   "source": [
    "`Ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7db4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "ollama.chat(model='llama3.2', messages=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content': 'Why is the sky blue?'\n",
    "    }\n",
    "])\n",
    "#response = ollama.generate(\"llama3.2\", \"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9b440",
   "metadata": {},
   "source": [
    "### `Google as a provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa68a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(api_key= os.environ[\"gemini_llm_api\"])\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d9b33",
   "metadata": {},
   "source": [
    "### `Open AI as a provider` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea757daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[답변]: content='대한민국의 수도는 서울입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 16, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CQRZ9YDX8HoM6RWrrXr3PS5rlpTy2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c63af1ab-2ce0-4545-bfeb-bef9f3fb1a6f-0' usage_metadata={'input_tokens': 16, 'output_tokens': 8, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    "    api_key=os.environ[\"openai_api_llm\"]\n",
    ")\n",
    "\n",
    "# 질의내용\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 질의\n",
    "print(f\"[답변]: {llm.invoke(question)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2be47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
